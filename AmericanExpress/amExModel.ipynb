{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amExModel.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trailblazerakash/MachineLearning/blob/master/AmericanExpress/amExModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jabx5snYUv2q",
        "colab_type": "code",
        "outputId": "770dbf7c-ddf0-4880-ee41-fa5edf241331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acFO1uRCbH37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "\n",
        "#import mlcrate as mlc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import lightgbm as lgb\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "import pickle as pkl\n",
        "import seaborn as sns\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
        "from keras import objectives\n",
        "from keras import backend as K\n",
        "from keras import regularizers \n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.layers import (Input, Lambda, Embedding, GaussianDropout, Reshape, CuDNNGRU,\n",
        "                          BatchNormalization, Dropout, Dense, PReLU, Layer,ReLU, LeakyReLU,GRU, Bidirectional)\n",
        "from keras.layers.merge import concatenate\n",
        "from sklearn.model_selection import KFold, GroupKFold\n",
        "from keras import callbacks\n",
        "from keras.layers import (Input, Lambda, Embedding, GaussianDropout, Reshape, CuDNNGRU,\n",
        "                          BatchNormalization, Dropout, Dense, PReLU, Layer,ReLU, LeakyReLU,GRU, Bidirectional)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5x2YV4yU71D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "campaign_data= pd.read_csv('/content/drive/My Drive/amExprt/train/campaign_data.csv')\n",
        "coupon_item_mapping=pd.read_csv('/content/drive/My Drive/amExprt/train/coupon_item_mapping.csv')\n",
        "customer_demographics=pd.read_csv('/content/drive/My Drive/amExprt/train/customer_demographics.csv')\n",
        "customer_transaction_data=pd.read_csv('/content/drive/My Drive/amExprt/train/customer_transaction_data.csv')\n",
        "item_data=pd.read_csv('/content/drive/My Drive/amExprt/train/item_data.csv')\n",
        "train=pd.read_csv('/content/drive/My Drive/amExprt/train/train.csv')\n",
        "\n",
        "\n",
        "test=pd.read_csv('/content/drive/My Drive/amExprt/test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReP6LQfcbF-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.concat([train, test], sort=False).reset_index(drop = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfZlESOqbO6h",
        "colab_type": "code",
        "outputId": "9cb4d4da-4f96-4579-cdf1-00ce2ed78fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>campaign_id</th>\n",
              "      <th>coupon_id</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>redemption_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>27</td>\n",
              "      <td>1053</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>116</td>\n",
              "      <td>48</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>635</td>\n",
              "      <td>205</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>644</td>\n",
              "      <td>1050</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>1017</td>\n",
              "      <td>1489</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  campaign_id  coupon_id  customer_id  redemption_status\n",
              "0   1           13         27         1053                0.0\n",
              "1   2           13        116           48                0.0\n",
              "2   6            9        635          205                0.0\n",
              "3   7           13        644         1050                0.0\n",
              "4   9            8       1017         1489                0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BajaMSriP-GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltr = len(train)\n",
        "data = data.merge(campaign_data, on='campaign_id')#  campaign_data\n",
        "data['start_date'] = pd.to_datetime(data['start_date'], dayfirst=True)\n",
        "data['end_date'] = pd.to_datetime(data['end_date'], dayfirst=True)\n",
        "data['campaign_type'] = pd.Series(data['campaign_type'].factorize()[0]).replace(-1, np.nan)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26HTmqB8QFur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "customer_demographics['no_of_children'] = customer_demographics['no_of_children'].replace('3+', 3).astype(float)\n",
        "customer_demographics['family_size'] = customer_demographics['family_size'].replace('5+', 3).astype(float)\n",
        "customer_demographics['marital_status'] = pd.Series(customer_demographics['marital_status'].factorize()[0]).replace(-1, np.nan)\n",
        "customer_demographics['age_range'] = pd.Series(customer_demographics['age_range'].factorize()[0]).replace(-1, np.nan)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtfKCZD0QY6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rented_mean = customer_demographics.groupby(\"customer_id\")['rented'].mean().to_dict()\n",
        "data['rented_mean'] = data['customer_id'].map(rented_mean)\n",
        "# income_bracket\n",
        "income_bracket_sum = customer_demographics.groupby(\"customer_id\")['income_bracket'].sum().to_dict()\n",
        "data['income_bracket_sum'] = data['customer_id'].map(income_bracket_sum)\n",
        "# age_range\n",
        "age_range_mean = customer_demographics.groupby(\"customer_id\")['age_range'].mean().to_dict()\n",
        "data['age_range_mean'] = data['customer_id'].map(age_range_mean)\n",
        "# family_size\n",
        "family_size_mean = customer_demographics.groupby(\"customer_id\")['family_size'].mean().to_dict()\n",
        "data['family_size_mean'] = data['customer_id'].map(family_size_mean)\n",
        "# no_of_children\n",
        "no_of_children_mean = customer_demographics.groupby(\"customer_id\")['no_of_children'].mean().to_dict()\n",
        "data['no_of_children_mean'] = data['customer_id'].map(no_of_children_mean)\n",
        "no_of_children_count = customer_demographics.groupby(\"customer_id\")['no_of_children'].count().to_dict()\n",
        "data['no_of_children_count'] = data['customer_id'].map(no_of_children_count)\n",
        "# marital_status\n",
        "marital_status_count = customer_demographics.groupby(\"customer_id\")['marital_status'].count().to_dict()\n",
        "data['marital_status_count'] = data['customer_id'].map(marital_status_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPLFu9pjQcsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "customer_transaction_data['date'] = pd.to_datetime(customer_transaction_data['date'])\n",
        "quantity_mean = customer_transaction_data.groupby(\"customer_id\")['quantity'].mean().to_dict()\n",
        "data['quantity_mean'] = data['customer_id'].map(quantity_mean)\n",
        "coupon_discount_mean = customer_transaction_data.groupby(\"customer_id\")['coupon_discount'].mean().to_dict()\n",
        "data['coupon_discount_mean'] = data['customer_id'].map(coupon_discount_mean)\n",
        "# other_discount\n",
        "other_discount_mean = customer_transaction_data.groupby(\"customer_id\")['other_discount'].mean().to_dict()\n",
        "data['other_discount_mean'] = data['customer_id'].map(other_discount_mean)\n",
        "# day\n",
        "customer_transaction_data['day'] = customer_transaction_data.date.dt.day\n",
        "date_day_mean = customer_transaction_data.groupby(\"customer_id\")['day'].mean().to_dict()\n",
        "data['date_day_mean'] = data['customer_id'].map(date_day_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Yl-5hCQhVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coupon_item_mapping = coupon_item_mapping.merge(item_data, how = 'left', on = 'item_id')\n",
        "coupon_item_mapping['brand_type'] = pd.Series(coupon_item_mapping['brand_type'].factorize()[0]).replace(-1, np.nan)\n",
        "coupon_item_mapping['category'] = pd.Series(coupon_item_mapping['category'].factorize()[0]).replace(-1, np.nan)\n",
        "\n",
        "category = coupon_item_mapping.groupby(\"coupon_id\")['category'].mean().to_dict()\n",
        "data['category_mean'] = data['coupon_id'].map(category)\n",
        "category = coupon_item_mapping.groupby(\"coupon_id\")['category'].count().to_dict()\n",
        "data['category_count'] = data['coupon_id'].map(category)\n",
        "category = coupon_item_mapping.groupby(\"coupon_id\")['category'].nunique().to_dict()\n",
        "data['category_nunique'] = data['coupon_id'].map(category)\n",
        "category = coupon_item_mapping.groupby(\"coupon_id\")['category'].max().to_dict()\n",
        "data['category_max'] = data['coupon_id'].map(category)\n",
        "category = coupon_item_mapping.groupby(\"coupon_id\")['category'].min().to_dict()\n",
        "data['category_min'] = data['coupon_id'].map(category)\n",
        "\n",
        "brand_mean = coupon_item_mapping.groupby(\"coupon_id\")['brand'].mean().to_dict()\n",
        "data['brand_mean'] = data['coupon_id'].map(brand_mean)\n",
        "brand_mean = coupon_item_mapping.groupby(\"coupon_id\")['brand'].count().to_dict()\n",
        "data['brand_count'] = data['coupon_id'].map(brand_mean)\n",
        "brand_mean = coupon_item_mapping.groupby(\"coupon_id\")['brand'].min().to_dict()\n",
        "data['brand_min'] = data['coupon_id'].map(brand_mean)\n",
        "brand_mean = coupon_item_mapping.groupby(\"coupon_id\")['brand'].max().to_dict()\n",
        "data['brand_max'] = data['coupon_id'].map(brand_mean)\n",
        "brand_mean = coupon_item_mapping.groupby(\"coupon_id\")['brand'].nunique().to_dict()\n",
        "data['brand_nunique'] = data['coupon_id'].map(brand_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jUngtGTQ19D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selling_price_mean = customer_transaction_data.groupby(\"customer_id\")['selling_price'].mean().to_dict()\n",
        "data['selling_price_mean'] = data['customer_id'].map(selling_price_mean)\n",
        "selling_price_mean = customer_transaction_data.groupby(\"customer_id\")['selling_price'].sum().to_dict()\n",
        "data['selling_price_sum'] = data['customer_id'].map(selling_price_mean)\n",
        "selling_price_mean = customer_transaction_data.groupby(\"customer_id\")['selling_price'].min().to_dict()\n",
        "data['selling_price_min'] = data['customer_id'].map(selling_price_mean)\n",
        "selling_price_mean = customer_transaction_data.groupby(\"customer_id\")['selling_price'].max().to_dict()\n",
        "data['selling_price_max'] = data['customer_id'].map(selling_price_mean)\n",
        "selling_price_mean = customer_transaction_data.groupby(\"customer_id\")['selling_price'].nunique().to_dict()\n",
        "data['selling_price_nunique'] = data['customer_id'].map(selling_price_mean)\n",
        "train_cols = [i for i in data.columns if i not in ['id','redemption_status','start_date','end_date']]\n",
        "train_cols = ['campaign_id','coupon_id','campaign_type','rented_mean','income_bracket_sum','age_range_mean','family_size_mean',\n",
        " 'no_of_children_mean',\n",
        " 'no_of_children_count',\n",
        " 'marital_status_count',\n",
        " 'quantity_mean',\n",
        " 'coupon_discount_mean',\n",
        " 'other_discount_mean',\n",
        " 'date_day_mean',\n",
        " 'category_mean',\n",
        " 'category_nunique',\n",
        " 'category_max',\n",
        " 'category_min',\n",
        " 'brand_mean',\n",
        " 'brand_max',\n",
        " 'brand_nunique',\n",
        " 'selling_price_mean',\n",
        " 'selling_price_min',\n",
        " 'selling_price_nunique']\n",
        "data[train_cols] = data[train_cols].fillna(0)\n",
        "train = data[data['redemption_status'].notnull()]\n",
        "test = data[data['redemption_status'].isnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcQgJJN7Q-FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fallback_auc(y_true, y_pred):\n",
        "    try:\n",
        "        return roc_auc_score(y_true, y_pred)\n",
        "    except:\n",
        "        return 0.5\n",
        "\n",
        "def auc(y_true, y_pred):\n",
        "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1DCc28LCINC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def init_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "    # The below is necessary for starting Numpy generated random numbers\n",
        "    # in a well-defined initial state.\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # The below is necessary for starting core Python generated random numbers\n",
        "    # in a well-defined state.\n",
        "\n",
        "    rn.seed(seed)\n",
        "\n",
        "    # Force TensorFlow to use single thread.\n",
        "    # Multiple threads are a potential source of\n",
        "    # non-reproducible results.\n",
        "    # For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
        "\n",
        "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "    from keras import backend as K\n",
        "\n",
        "    # The below tf.set_random_seed() will make random number generation\n",
        "    # in the TensorFlow backend have a well-defined initial state.\n",
        "    # For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
        "\n",
        "    tf.set_random_seed(seed)\n",
        "\n",
        "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "    K.set_session(sess)\n",
        "    return sess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtgWoZwxCK8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "46e04554-df2b-4933-b489-2bc90bb286fd"
      },
      "source": [
        "\n",
        "f_size  = [int(np.absolute(data[f]).max()) + 1 for f in train_cols]\n",
        "k_latent = 2\n",
        "embedding_reg = 0.0002\n",
        "kernel_reg = 0.1\n",
        "\n",
        "def get_embed(x_input, x_size, k_latent):\n",
        "    if x_size > 0: #category\n",
        "        embed = Embedding(x_size, k_latent, input_length=1, \n",
        "                          embeddings_regularizer=l2(embedding_reg))(x_input)\n",
        "        embed = Flatten()(embed)\n",
        "    else:\n",
        "        embed = Dense(k_latent, kernel_regularizer=l2(embedding_reg))(x_input)\n",
        "    return embed\n",
        "\n",
        "def build_model_1(X, f_size):\n",
        "    dim_input = len(f_size)\n",
        "    \n",
        "    input_x = [Input(shape=(1,)) for i in range(dim_input)] \n",
        "     \n",
        "    biases = [get_embed(x, size, 2) for (x, size) in zip(input_x, f_size)]\n",
        "    \n",
        "    factors = [get_embed(x, size, k_latent) for (x, size) in zip(input_x, f_size)]\n",
        "    \n",
        "    s = Add()(factors)\n",
        "    \n",
        "    diffs = [Subtract()([s, x]) for x in factors]\n",
        "    \n",
        "    dots = [Dot(axes=1)([d, x]) for d,x in zip(diffs, factors)]\n",
        "    \n",
        "    x = Concatenate()(biases + dots)\n",
        "    x = BatchNormalization()(x)\n",
        "    output = Dense(1, activation='sigmoid', kernel_regularizer=l2(kernel_reg))(x)\n",
        "    model = Model(inputs=input_x, outputs=[output])\n",
        "    opt = Adam(clipnorm=0.2, lr=0.0031)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[auc])\n",
        "    output_f = factors + biases\n",
        "    model_features = Model(inputs=input_x, outputs=output_f)\n",
        "    return model, model_features\n",
        "n_epochs = 100\n",
        "P = 10\n",
        "batch_size = 2**P\n",
        "print(batch_size)\n",
        "\n",
        "earlystopper = EarlyStopping(patience=0, verbose=1)\n",
        "kf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 228)\n",
        "# kf = GroupKFold(5)\n",
        "\n",
        "score = []\n",
        "prediction = np.zeros(len(test))\n",
        "validate = np.zeros(len(train))\n",
        "\n",
        "test_ = [np.absolute(test[f].values) for f in train_cols]\n",
        "y_train = train.redemption_status.values\n",
        "w_train = (30 * (y_train > 0).astype('float32') + 1).ravel()\n",
        "\n",
        "def schedule(epoch, lr):\n",
        "    if epoch <= 10:\n",
        "        lr = 0.0031\n",
        "    if epoch > 10:\n",
        "        lr = lr * 0.8\n",
        "    return lr\n",
        "lr_s = callbacks.LearningRateScheduler(schedule, verbose=1)\n",
        "pred = pd.DataFrame()\n",
        "for i , (tdx, vdx) in enumerate(kf.split(train, train.redemption_status)):\n",
        "    try:\n",
        "        del sess\n",
        "    except:\n",
        "        pass\n",
        "    sess = init_seeds(i)\n",
        "        \n",
        "    print(f\"FOLD : {i}\")\n",
        "    X_train = [np.absolute(train[f].iloc[tdx].values) for f in train_cols]\n",
        "    X_test = [np.absolute(train[f].iloc[vdx].values) for f in train_cols]\n",
        "    model, model_features = build_model_1(X_train, f_size)\n",
        "    csv_logger = callbacks.CSVLogger(f'training_focal_loss{i}.log')\n",
        "    model.fit(X_train,  y_train[tdx], \n",
        "          epochs=n_epochs, batch_size=batch_size, verbose=2, shuffle=True, \n",
        "          validation_data=(X_test,  y_train[vdx]), \n",
        "          callbacks=[earlystopper, csv_logger],\n",
        "\n",
        "         )\n",
        "    \n",
        "    pred[str(i)] = model.predict(test_,verbose = False,batch_size=batch_size).reshape(-1)\n",
        "    validate[vdx] = model.predict(X_test).reshape(-1)\n",
        "    \n",
        "    print(roc_auc_score(y_train[vdx], validate[vdx]))\n",
        "    score.append(roc_auc_score(y_train[vdx], validate[vdx]))\n",
        "    model.save_weights(f\"model{i}.h5\")\n",
        "    del X_train, X_test,model, model_features\n",
        "    gc.collect()\n",
        "    \n",
        "print(score)\n",
        "print(f\"CV : {np.mean(score)}\")\n",
        "    \n",
        "    \n",
        "tmp = pred.copy()\n",
        "for col in tmp.columns:\n",
        "    tmp[col] = tmp[col].rank()\n",
        "    \n",
        "tmp = tmp.mean(axis = 1)\n",
        "tmp  =tmp / tmp.max()\n",
        "day = 4\n",
        "sub = 5\n",
        "name = f\"day_{day}_sub_{sub}\"\n",
        "tmp = dict(zip(test.id.values, tmp))\n",
        "answer1 = pd.DataFrame()\n",
        "answer1['id'] = test.id.values\n",
        "answer1['redemption_status'] = answer1['id'].map(tmp)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1024\n",
            "FOLD : 0\n",
            "Train on 70532 samples, validate on 7837 samples\n",
            "Epoch 1/100\n",
            " - 24s - loss: 0.4757 - auc: 0.7685 - val_loss: 0.2154 - val_auc: 0.8841\n",
            "Epoch 2/100\n",
            " - 2s - loss: 0.1767 - auc: 0.9048 - val_loss: 0.1338 - val_auc: 0.8971\n",
            "Epoch 3/100\n",
            " - 2s - loss: 0.1287 - auc: 0.9345 - val_loss: 0.1115 - val_auc: 0.8879\n",
            "Epoch 4/100\n",
            " - 2s - loss: 0.1080 - auc: 0.9458 - val_loss: 0.0987 - val_auc: 0.9021\n",
            "Epoch 5/100\n",
            " - 2s - loss: 0.0951 - auc: 0.9521 - val_loss: 0.0905 - val_auc: 0.8946\n",
            "Epoch 6/100\n",
            " - 2s - loss: 0.0864 - auc: 0.9599 - val_loss: 0.0845 - val_auc: 0.9081\n",
            "Epoch 7/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0VIDmGjCSIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer1.to_csv(f'/content/drive/My Drive/amExprt/{name}.csv', index = None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}